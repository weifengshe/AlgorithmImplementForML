/////Hard Grid World Analysis/////

This is your grid world:
[0,1,0,0,0,1,0,0,0,0,0]
[1,0,0,0,0,0,0,0,1,0,0]
[0,1,1,1,0,1,0,0,0,1,0]
[0,0,0,1,0,1,1,1,0,0,0]
[0,0,0,1,0,0,0,1,0,0,0]
[0,0,0,0,0,1,0,1,0,1,0]
[0,0,0,0,0,1,0,1,1,0,1]
[0,0,0,1,1,1,0,0,0,1,0]
[0,0,0,0,0,1,0,0,0,1,0]
[0,0,0,0,0,1,1,1,0,1,0]
[0,0,0,0,0,0,0,0,0,0,0]



//Q Learning Analysis//
Q Learning,478,560,504,450,416,464,63,37,65,141,210,208,460,340,221,213,61,44,146,59,92,46,224,49,325,332,63,50,65,81,101,28,68,40,57,56,98,179,96,152,37,37,27,36,172,86,40,122,106,116,33,29,79,39,129,65,33,77,107,68,330,26,42,45,58,33,58,58,43,29,101,45,67,39,74,125,42,48,45,55,42,56,127,65,149,52,29,70,62,39,129,80,42,47,78,65,58,49,72,35
Passes: 26

This is your optimal policy:
num of rows in policy is 11
[*,*,>,>,^,*,>,>,^,>,^]
[*,^,>,>,>,>,>,^,*,v,>]
[v,*,*,*,^,*,>,^,>,*,^]
[>,<,^,*,^,*,*,*,>,v,^]
[^,v,v,*,^,^,^,*,v,^,>]
[v,>,^,>,^,*,^,*,>,*,<]
[>,v,>,>,^,*,v,*,*,*,*]
[>,^,^,*,*,*,<,v,v,*,v]
[^,^,<,v,>,*,v,v,<,*,>]
[>,^,^,<,<,*,*,*,<,*,<]
[v,^,<,^,>,>,>,>,<,<,<]



//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,
Policy Iteration,
Q Learning,478,560,504,450,416,464,63,37,65,141,210,208,460,340,221,213,61,44,146,59,92,46,224,49,325,332,63,50,65,81,101,28,68,40,57,56,98,179,96,152,37,37,27,36,172,86,40,122,106,116,33,29,79,39,129,65,33,77,107,68,330,26,42,45,58,33,58,58,43,29,101,45,67,39,74,125,42,48,45,55,42,56,127,65,149,52,29,70,62,39,129,80,42,47,78,65,58,49,72,35

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,
Policy Iteration,
Q Learning,182,49,37,90,43,43,40,97,46,58,39,68,104,47,45,53,50,39,57,132,121,46,51,50,46,32,29,32,39,103,101,70,31,38,52,45,50,89,38,42,54,54,52,50,34,56,47,99,50,65,44,34,32,45,51,43,61,47,71,43,60,53,59,52,59,55,43,51,54,54,41,56,40,46,68,49,47,57,52,50,64,83,110,74,65,104,76,64,59,75,80,83,66,45,76,97,102,59,65,55

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-376.0,-458.0,-402.0,-348.0,-314.0,-362.0,39.0,65.0,37.0,-39.0,-108.0,-106.0,-358.0,-238.0,-119.0,-111.0,41.0,58.0,-44.0,43.0,10.0,56.0,-122.0,53.0,-223.0,-230.0,39.0,52.0,37.0,21.0,1.0,74.0,34.0,62.0,45.0,46.0,4.0,-77.0,6.0,-50.0,65.0,65.0,75.0,66.0,-70.0,16.0,62.0,-20.0,-4.0,-14.0,69.0,73.0,23.0,63.0,-27.0,37.0,69.0,25.0,-5.0,34.0,-228.0,76.0,60.0,57.0,44.0,69.0,44.0,44.0,59.0,73.0,1.0,57.0,35.0,63.0,28.0,-23.0,60.0,54.0,57.0,47.0,60.0,46.0,-25.0,37.0,-47.0,50.0,73.0,32.0,40.0,63.0,-27.0,22.0,60.0,55.0,24.0,37.0,44.0,53.0,30.0,67.0
