---
title: "UL 3 Feature Selection"

author: "Weifeng She"

output: html_document
---

#### introduction

* Knowledge Discovery: interpretability & insight (only a few matter)
* Curse of Dimensionality

#### Feature Selection: Algorithm

starting with n features, to get m features 

N features -> m features for  m <= n

How hard is the problem: exponential NP-hard

two ways to tackle: Filtering and wrapping

Filtering:set of features as input,  passed in certain search algorithm, output some features and passed in learning aglorithm

Wrapping: take the set of features, search over subset of features, ask learning algorithm to do something on it, 
learning algorithm reports how well it does, use that to update the new subset feature it might look for 
because search wrapping around learning algorithm

Filtering: no feedback

filter:

pros : speed, it is faster than wrapping, by making approximation,

cons: looking feature as isolation and ignore the learning problem

filtering search algorithm:  decision tree could be used as filter search algorithm and give you a subset of the feature

wrapping: 

pros: takes into account model bias and learning 

cons: much slower

* feature selection criteria: 

information gain(conditional entropy, depend on labels),

variance, entropy(doesn't depond on labels), "useful" features, independent/non-redondant features

don't take advantage of the learner

* algorithms can be used by wrapping:

hill climbing, 
randomized optimization,
forward search and backward search

#### Relevance vs usefulness

* Relevance measures effect on bayes optimmal classifier, relevance is all about informaiton

* usefullness measures effect on a particular predictor


* feature selection 

* filtering vs Wrapping, wrapping is slow and useful, filter is faster ignores bias

* relevance: things give you information about the classification you care about. useful: things help you learning given some specific algorithms

* strong and weak relevance

