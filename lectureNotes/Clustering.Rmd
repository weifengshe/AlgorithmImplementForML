---
title: "Unsupervised learning 2 - CLUSTERING"
author: "Weifeng She"
output: html_document
---


* Supervised learning: use labeled training data to generalize labels to new instancesï¼Œ kind of Function approximation

* Unsupervised learning: make sense out of unlabeled data, kind of Data Description

#### Basic Clustering Problem

* Given: set of objects of X, (information how they related to each other) inter-object distance D(x, y) = D(y, x), x,y belong to X

* Output: Partition $P_D(x)$ = $P_D(y)$ if x and y in same cluster

#### single linkage clustering (SLC)

* consider each object a cluster (n objects)
* define intercluster distance as the distance between the closest two point in the two cluster, 

distance is somewhere domain knowledge comes in

* merge two closest clusters 
* repeat n-k times to make k clusters

slc can form a harachical aggretative cluster structure

#### Running time of SLC
n points (objects)
K clusters

how to best characterize the running time of single-link clustering?

1. repeat k times(n/2)

2. at each step look at all distance to find closest point $O(n^2)$ that have different labels

the total running time: $O(n^3)$ 

Issue with SLC: sometimes could not find the best cluster

#### K-means clustering

* pick K centers (at random)
* each center "claims" its closest points 
* recompute the centers by avearging the clustered points
* repeat until convergence

#### K means in Euclidean space part 

* $P^t(x)$: partition/cluster of object x.

* $C_i^t$ : set of all points in cluster i = {x s.t.  P(x) = i} (set of x, P(x) equal to i)

* $$Center_i^t = \sum_{y \in C_i^t} y/ |C_i|$$  

centroid:

$$Center_i^0 \rightarrow$$ 

$$P^t(x) = argmin{i}||X-Center_i^\{t-1}||_2^2 \rightleftarrow$$

$$Center_i^t = \sum_{y \in C_i^t} y/ |C_i|$$ 
Partition: for each point, loop over all the clusters, find the cluster whose center is closeset to x in terms of squared distance, What happen to E, when we do that, we move one point for ont cluster to another, only if it cause the squared distance to the center goes down. 
Center: when we move the center, average minimizes squared error. 

If I have finite configurations, if I always break ties consistently, and I never go to the configuration with a higher error, if I don't repeat configuration, then at some point I will run out of configuration, then converge.

$k^n$ configuration. 
Monotonically non-increasing in error

#### K-means as optimization
* Configurations (input)-center, Partition (should be good representation of your data, unsupervised learning as compact representation)
* Scores - a good representation is how much error you introduced by representing these points as partitions or in this case as centers
$$E(P, Center) = \sum_{x} || Center_{P(x)} -x||_2^2$$ (x must in the same cluster as center)
* neighborhood(move from configuration to configuration to improve)
$$P, Center = {(p^p, Center)} \cup {(P, Center^p)}$$ 
each time either change the partition of center

Hillclimbing is most like K-means clustering,
each iteration, error only can go down, monotonically non-increasing in error, since there are finite number of configurations, finite number of labels they can have, should have some ways to break ties if a point have equal chance to go to different clusters 

#### Properties of K-means clustering

* each iteration: polynomial $O(kn)$
* finite (exponential ) iterations $O(k^n)$
* error decreases(if ties broken consistently) [with one exception]
* can get stuck! can be solved by random restart, or pick cluster center spread-out

#### soft clustering
lean on probability theorem

Assume the data was generated by 

1. Select one of k Gaussians [And known variance $\sigma^2$] uniformly

2. Sample $x_i$ (data point) from that Gaussian

3. Repeat n times
Task: find a hypothesis  h = <$\mu_1$, $\mu_2$, ..., $\mu_k$> that maximizes the probability of the data(ML, amximum likelihood)

### Maximum Likelihood Gaussian

The ML mean of the Gaussian $\mu$ is the mean of the data!

What if we have K of them? answer is Hdden variables!

<x, $z_1$, $z_2$, ..., $z_k$> # z indicate which clustering x come from, we need to do some inference to figure out what those values are

### Expectation Maximization 

soft clustering computing the expectation: 

$$E[z_{ij}] = P(x = x_i|\mu = \mu_j)/(\sum_{i=1}^k P(x= x_i| \mu = \mu_j)) $$
likelihood of data element i comes from cluster j: to use bayes rule, say that will be proportional to probabilty of data i produced by j, then normalized. 

Expectation (define z from $\mu$)

then we pass the clustering information z to maximazation step to compute the mean of the clusters

$\mu_j = \sum_{i} (E[z_{ij}]x_i)/\sum_{i}(E[z_ij])$

if we know the cluster information over to maximation step, calculate the mean from the clusters, we can take the average variable value, then average of x_i in each cluster j, and we have normalize

Maximization (define $\mu$ from z)

#### Properties of EM

* monotonically non-decreasing likelihood
* does not converge (practically does). In EM, configuration is possibilities, there are infinite number of it, you can move closer, but never reach
* will not diverge
* can get stuck (local optimal, sovle by random restart!)
* works with any distribution (if E.M. suluable: Bayes net, counting things)
small matter of mathematical programming

#### clustering properties

* Richness: for any assignment of objects to clusters, there is some distance matrix D such that P returns that clusttering $\forall C,  \exists P_D$ = C
* Scale-invariance, scaling distances by a positive value does not change the clustering
$\forall D, \forall_{k\greater than0} p_D = P_{kD}$
* Consistency: shrinking intracluster distances and expending intercluster distance does not cahnge the clustering $$P_D = P^p$$

### impossibility Theorem  by Kleinberg

No clustering scheme can achieve all three of:

* richness

* scale invariance

* consistency





